<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>SignSpeak | Real-time Sign Language Translation</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Fraunces:opsz,wght@9..144,600;9..144,700&family=Space+Grotesk:wght@400;500;600&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="styles.css" />
  </head>
  <body>
    <div class="bg-orbs" aria-hidden="true">
      <span class="orb orb-1"></span>
      <span class="orb orb-2"></span>
      <span class="orb orb-3"></span>
    </div>

    <header class="nav">
      <div class="brand">
        <span class="brand-mark">SS</span>
        <span class="brand-name">SignSpeak</span>
      </div>
      <nav class="nav-links">
        <a href="#how" class="nav-link">How it works</a>
        <a href="#features" class="nav-link">Features</a>
        <a href="#deploy" class="nav-link">Deploy</a>
      </nav>
        <a
          class="btn btn-ghost"
          href="https://github.com/rahimshah-dev/AI_Sign_Language_Translation"
          target="_blank"
          rel="noreferrer"
        >
          GitHub
        </a>
    </header>

    <main>
      <section class="hero" data-reveal>
        <div class="hero-copy">
          <p class="eyebrow">Real-time gesture intelligence</p>
          <h1>Translate Sign Language in Real Time.</h1>
          <p class="lead">
            A fast, lightweight pipeline powered by MediaPipe and OpenCV. Capture
            hand landmarks, train a compact classifier, and run live inference
            in milliseconds.
          </p>
          <div class="cta-row">
            <a href="#deploy" class="btn btn-primary">Run locally</a>
            <a
              href="https://github.com/rahimshah-dev/AI_Sign_Language_Translation"
              target="_blank"
              rel="noreferrer"
              class="btn btn-outline"
            >
              Get the code
            </a>
          </div>
          <div class="stats">
            <div class="stat">
              <span class="stat-value">21</span>
              <span class="stat-label">Landmarks per hand</span>
            </div>
            <div class="stat">
              <span class="stat-value">2</span>
              <span class="stat-label">Hands tracked</span>
            </div>
            <div class="stat">
              <span class="stat-value">5+</span>
              <span class="stat-label">Gesture classes</span>
            </div>
          </div>
        </div>
        <div class="hero-panel">
          <div class="panel-header">
            <span class="pulse"></span>
            Live preview
          </div>
          <div class="panel-body">
            <div class="video-shell">
              <video id="video" autoplay playsinline muted></video>
              <canvas id="overlay"></canvas>
            </div>
            <div class="panel-label">Prediction: <strong id="prediction">--</strong></div>
            <div class="panel-status" id="status">Loading modelâ€¦</div>
          </div>
        </div>
      </section>

      <section id="features" class="section features" data-reveal>
        <div class="section-title">
          <h2>Built for speed and clarity</h2>
          <p>
            The pipeline keeps the moving parts small so you can iterate on new
            gestures quickly.
          </p>
        </div>
        <div class="feature-grid">
          <div class="card">
            <h3>Landmark Precision</h3>
            <p>
              MediaPipe extracts 21 keypoints per hand for stable, repeatable
              feature vectors.
            </p>
          </div>
          <div class="card">
            <h3>Lightweight Model</h3>
            <p>
              Random Forest training keeps inference fast without expensive GPU
              requirements.
            </p>
          </div>
          <div class="card">
            <h3>Real-time Feedback</h3>
            <p>
              On-screen predictions make it easy to validate each sign while you
              capture data.
            </p>
          </div>
        </div>
      </section>

      <section id="how" class="section pipeline" data-reveal>
        <div class="section-title">
          <h2>From webcam to prediction</h2>
          <p>Collect, train, and deploy in a tight loop.</p>
        </div>
        <ol class="steps">
          <li>
            <span class="step-count">01</span>
            Capture labeled images for each sign.
          </li>
          <li>
            <span class="step-count">02</span>
            Extract landmarks into `data.pickle`.
          </li>
          <li>
            <span class="step-count">03</span>
            Train a classifier and save `model.p`.
          </li>
          <li>
            <span class="step-count">04</span>
            Run live inference with on-screen predictions.
          </li>
        </ol>
      </section>

      <section id="deploy" class="section deploy" data-reveal>
        <div class="deploy-card">
          <div>
            <h2>Deploy-ready, open source</h2>
            <p>
              Pair this frontend with your Python pipeline and ship a clean
              landing page for demos, class projects, or client work.
            </p>
            <div class="cta-row">
              <a
                href="https://github.com/rahimshah-dev/AI_Sign_Language_Translation"
                target="_blank"
                rel="noreferrer"
                class="btn btn-primary"
              >
                View on GitHub
              </a>
              <a href="#install" class="btn btn-ghost">Setup guide</a>
            </div>
          </div>
          <div class="deploy-note">
            <p class="note-title">Quick start</p>
            <pre id="install"><code>python3 -m venv .venv
.venv/bin/python -m pip install -r requirements.txt
HAND_LANDMARKER_MODEL=./hand_landmarker.task .venv/bin/python inference_classifier.py</code></pre>
          </div>
        </div>
      </section>
    </main>

    <footer class="footer">
      <p>Made for rapid gesture experiments. Open-source and ready to extend.</p>
      <a
        href="https://github.com/rahimshah-dev/AI_Sign_Language_Translation"
        target="_blank"
        rel="noreferrer"
        class="footer-link"
      >
        Get the code on GitHub
      </a>
    </footer>

    <script type="module" src="app.js"></script>
  </body>
</html>
